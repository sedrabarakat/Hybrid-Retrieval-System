import sys
import os
from pathlib import Path # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Path

# ØªØµØ­ÙŠØ­ Ø­Ø³Ø§Ø¨ project_root
# Ù‡Ø°Ø§ Ø³ÙŠØ£Ø®Ø°Ùƒ Ù…Ù† C:\Users\HP\IR-project\query_processing\with_clustring\ranking_clustring.py
# Ø¥Ù„Ù‰ C:\Users\HP\IR-project\
# ØªÙ… ØªØºÙŠÙŠØ± parents[3] Ø¥Ù„Ù‰ parents[2]
project_root = Path(__file__).resolve().parents[2] 
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import numpy as np
from collections import OrderedDict
from sklearn.metrics.pairwise import cosine_similarity
import time # ØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø© Ø§Ù„ÙˆÙ‚Øª

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ load_clusters Ù‡Ù†Ø§
from storage.vector_storage import load_tfidf_matrix, load_doc_ids, load_clusters 
from indexing.inverted_index_loader import load_inverted_index
from tf.query_processing import QueryProcessor

import mysql.connector

# ØªØµØ­ÙŠØ­ Ø­Ø³Ø§Ø¨ project_root
project_root = Path(__file__).resolve().parents[2] 
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Ø§Ù„Ø­Ù„ Ù„Ù…Ø´ÙƒÙ„Ø© 'No module named 'TF_IDF''
import vectorize.tokenizer_definition 
sys.modules["TF_IDF"] = vectorize.tokenizer_definition


class IndexDataLoader:
    _cache = {} # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ø´ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙØ¦Ø©

    def __init__(self):
        pass # Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„ØªÙ‡ÙŠØ¦Ø© Ù‡Ù†Ø§

    def load(self, dataset_name):
        if dataset_name in self._cache:
            print(f"[CACHE] Using cached index data for {dataset_name}")
            return self._cache[dataset_name]

        print(f"[DISK] Loading index data for {dataset_name} from disk...")
        tfidf_matrix = load_tfidf_matrix(f"{dataset_name}_all")
        doc_ids = load_doc_ids(f"{dataset_name}_all")
        inverted_index = load_inverted_index(dataset_name)
        
        # **ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù‡Ù†Ø§**
        clusters_data = load_clusters(f"{dataset_name}_all") 

        # **Ù†Ù‚Ù„ Ø¥Ù†Ø´Ø§Ø¡ doc_id_to_index Ø¥Ù„Ù‰ Ù‡Ù†Ø§ ÙˆØªØ®Ø²ÙŠÙ†Ù‡ Ù…Ø¤Ù‚ØªÙ‹Ø§**
        print(f"Creating doc_id_to_index map for {len(doc_ids)} documents...")
        doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}
        print("Doc_id_to_index map created.")

        self._cache[dataset_name] = {
            'tfidf_matrix': tfidf_matrix,
            'doc_ids': doc_ids,
            'inverted_index': inverted_index,
            'clusters': clusters_data, # Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ø´
            'doc_id_to_index': doc_id_to_index # Ø¥Ø¶Ø§ÙØ© doc_id_to_index Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ø´
        }
        print(f"[DISK] Index data for {dataset_name} loaded and cached.")
        return self._cache[dataset_name]


index_loader = IndexDataLoader() # Ø£Ù†Ø´Ø¦ loader Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© (global)


def load_documents_text(dataset_name, doc_ids, batch_size=1000):
    """
    ÙŠØ­Ù…Ù„ Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø±Ù‘ÙØ§ØªÙ‡Ø§.
    ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ù„Ø¨ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.
    """
    if not doc_ids:
        return {}

    conn = mysql.connector.connect(
        host="localhost",
        user="root",
        password="",
        database="ir"
    )
    cursor = conn.cursor(dictionary=True)

    all_rows = []
    for i in range(0, len(doc_ids), batch_size):
        batch_ids = doc_ids[i:i+batch_size]
        placeholders = ','.join(['%s'] * len(batch_ids))
        query = f"""
            SELECT id, text
            FROM documents
            WHERE id IN ({placeholders})
              AND dataset_name = %s
        """
        cursor.execute(query, (*batch_ids, dataset_name))
        rows = cursor.fetchall()
        all_rows.extend(rows)

    conn.close()
    return {row['id']: row['text'] for row in all_rows}


def match_and_rank(query: str, dataset_name: str, similarity_threshold=0.0001, top_k=None, use_clusters: bool = False):
    """
    ÙŠÙ‚ÙˆÙ… Ø¨Ù…Ø·Ø§Ø¨Ù‚Ø© ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ÙŠÙ†ØŒ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª.

    Parameters:
    - query (str): Ù†Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….
    - dataset_name (str): Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
    - similarity_threshold (float): Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯.
    - top_k (int, optional): Ø¹Ø¯Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§.
    - use_clusters (bool): Ø¥Ø°Ø§ ÙƒØ§Ù†Øª TrueØŒ Ø³ÙŠØªÙ… ØªØµÙÙŠØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….
    """
    start_time_total = time.time() 

    print(f"Starting match and rank for query: '{query}' on dataset: '{dataset_name}' (Clusters enabled: {use_clusters})")
    
    qp = QueryProcessor(dataset_name)
    query_vector, tokens = qp.process(query)
    print(f"Query processed. Tokens: {tokens}")

    data = index_loader.load(dataset_name)
    tfidf_matrix = data['tfidf_matrix']
    doc_ids = data['doc_ids']
    inverted_index = data['inverted_index']
    clusters_data = data['clusters'] # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
    doc_id_to_index = data['doc_id_to_index'] # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ doc_id_to_index Ù…Ù† Ø§Ù„ÙƒØ§Ø´

    matched_tokens = [t for t in tokens if t in inverted_index]
    if not matched_tokens:
        print("[!] Ù„Ù… ØªÙÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        end_time_total = time.time()
        print(f"Total match and rank execution time: {end_time_total - start_time_total:.2f} seconds.")
        return OrderedDict()

    candidate_doc_ids = set()
    for token in matched_tokens:
        candidate_doc_ids.update(inverted_index[token])

    # **ØªØ·Ø¨ÙŠÙ‚ Ù…Ù†Ø·Ù‚ ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ use_clusters**
    filtered_candidate_indices = []
    if use_clusters:
        print("Determining relevant clusters for the query...")
        cluster_relevance_counts = np.zeros(clusters_data.max() + 1)
        for token in matched_tokens:
            if token in inverted_index:
                doc_indices_for_token = [doc_id_to_index[d_id] for d_id in inverted_index[token] if d_id in doc_id_to_index]
                for doc_idx in doc_indices_for_token:
                    cluster_id = clusters_data[doc_idx]
                    cluster_relevance_counts[cluster_id] += 1
        
        if cluster_relevance_counts.sum() == 0:
            print("[!] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
            relevant_clusters = None 
        else:
            most_relevant_cluster_id = np.argmax(cluster_relevance_counts)
            relevant_clusters = [most_relevant_cluster_id] 
            print(f"Query most relevant to cluster(s): {relevant_clusters}")

        print(f"Filtering candidate documents to relevant clusters: {relevant_clusters}")
        if relevant_clusters is not None:
            for doc_id in candidate_doc_ids:
                if doc_id in doc_id_to_index:
                    doc_idx = doc_id_to_index[doc_id]
                    if clusters_data[doc_idx] in relevant_clusters:
                        filtered_candidate_indices.append(doc_idx)
        else: # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø°Ø§Øª ØµÙ„Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
            filtered_candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]
    else: # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª use_clusters FalseØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³
        print("Clusters disabled. Searching across all candidate documents.")
        filtered_candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]


    if not filtered_candidate_indices:
        print("[!] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ© Ø¨Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        end_time_total = time.time()
        print(f"Total match and rank execution time: {end_time_total - start_time_total:.2f} seconds.")
        return OrderedDict()

    filtered_candidate_indices = sorted(filtered_candidate_indices)
    candidate_vectors = tfidf_matrix[filtered_candidate_indices]

    print(f"Calculating cosine similarity for {len(filtered_candidate_indices)} candidate documents...")
    similarity_scores = cosine_similarity(query_vector, candidate_vectors).flatten()

    ranking = {
        doc_ids[i]: float(score)
        for i, score in zip(filtered_candidate_indices, similarity_scores)
        if score >= similarity_threshold
    }

    sorted_ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)

    if top_k:
        sorted_ranking = sorted_ranking[:top_k]

    top_doc_ids = [doc_id for doc_id, _ in sorted_ranking]
    print(f"Loading text for top {len(top_doc_ids)} documents...") 
    documents_texts = load_documents_text(dataset_name, top_doc_ids) 

    print(f"[14] Ø¹Ø±Ø¶ Ø£Ø¹Ù„Ù‰ {top_k if top_k else 'Ø§Ù„ÙƒÙ„'} Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªØ¨Ø© Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:")
    for rank, (doc_id, score) in enumerate(sorted_ranking[:5], 1):
        text = documents_texts.get(doc_id, "[Ø§Ù„Ù†Øµ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯]") 
        print(f"ğŸ”¹ Rank: {rank}")
        print(f"   Doc ID: {doc_id}")
        print(f"   Score: {score:.6f}")
        print(f"   Text: {text[:200]}...") 
        print("-" * 50)

    end_time_total = time.time()
    print(f"Match and rank completed. Found {len(sorted_ranking)} relevant documents.")
    print(f"Total match and rank execution time: {end_time_total - start_time_total:.2f} seconds.")
    return OrderedDict(sorted_ranking)

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (ÙÙŠ Ù…Ù„ÙÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø£Ùˆ Ù…Ù„Ù Ø§Ù„ØªØ´ØºÙŠÙ„):
# match_and_rank(query="What is the capital of France?", dataset_name="beir", top_k=10, use_clusters=True)

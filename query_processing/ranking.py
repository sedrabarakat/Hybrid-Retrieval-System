import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
import numpy as np
from collections import OrderedDict
from sklearn.metrics.pairwise import cosine_similarity
from storage.vector_storage import load_tfidf_matrix, load_doc_ids
from indexing.inverted_index_loader import load_inverted_index
from query_processing import QueryProcessor



def match_and_rank(query: str, dataset_name: str, similarity_threshold=0.0001, top_k=None):
    print("[1] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…...")
    qp = QueryProcessor(dataset_name)
    query_vector, tokens = qp.process(query)
    print(f"[2] ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {tokens}")

    print("[3] ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF...")
    tfidf_matrix = load_tfidf_matrix(f"{dataset_name}_all")
    print(f"[4] Ø´ÙƒÙ„ Ù…ØµÙÙˆÙØ© TF-IDF: {tfidf_matrix.shape}")

    print("[5] ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
    doc_ids = load_doc_ids(f"{dataset_name}_all")
    print(f"[6] Ø¹ÙŠÙ†Ø© Ù…Ù† doc_ids: {doc_ids[:5]} (ÙƒÙ„Ù‡Ø§ Ù…Ù† Ù†ÙˆØ¹: {type(doc_ids[0])})")

    print("[7] ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³...")
    inverted_index = load_inverted_index(dataset_name)
    print(f"[8] Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {len(inverted_index)}")

    matched_tokens = [t for t in tokens if t in inverted_index]
    print(f"[9] Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {matched_tokens}")

    if not matched_tokens:
        print("[!] Ù„Ù… ØªÙÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        return OrderedDict()

    candidate_doc_ids = set()
    for token in matched_tokens:
        candidate_doc_ids.update(inverted_index[token])
    print(f"[10] Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ: {len(candidate_doc_ids)}")

    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}
    print(f"[11] ØªÙ… Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨ÙŠÙ† doc_id Ùˆ index.")

    print("ðŸ” Ø¹ÙŠÙ†Ø© Ù…Ù† candidate_doc_ids (ÙÙ‡Ø±Ø³ Ù…Ø¹ÙƒÙˆØ³):", list(candidate_doc_ids)[:10])
    print("ðŸ” Ø¹ÙŠÙ†Ø© Ù…Ù† doc_ids Ø§Ù„Ù…Ø­Ù…Ù„Ø©:", doc_ids[:10])

    missing_doc_ids = [doc_id for doc_id in candidate_doc_ids if doc_id not in doc_id_to_index]
    print(f"ðŸ” Ø¹Ø¯Ø¯ doc_ids ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ doc_id_to_index: {len(missing_doc_ids)}")
    print("ðŸ” Ø¹ÙŠÙ†Ø© Ù…Ù†Ù‡Ø§:", missing_doc_ids[:10])

    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]
    print(f"[12] Ø¹Ø¯Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {len(candidate_indices)}")

    if not candidate_indices:
        print("[!] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø·Ø§Ø¨Ù‚Ø©. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        return OrderedDict()

    candidate_indices = sorted(candidate_indices)
    candidate_vectors = tfidf_matrix[candidate_indices]
    print(f"[13] ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø©. Ø§Ù„Ø´ÙƒÙ„: {candidate_vectors.shape}")

    similarity_scores = cosine_similarity(query_vector, candidate_vectors).flatten()
    print(f"[14] ØªÙ… Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡. Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: {len(similarity_scores)}")

    ranking = {
        doc_ids[i]: float(score)
        for i, score in zip(candidate_indices, similarity_scores)
        if score >= similarity_threshold
    }
    print(f"[15] Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ({similarity_threshold}): {len(ranking)}")

    sorted_ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)

    if top_k:
        sorted_ranking = sorted_ranking[:top_k]
    print(f"[16] Ø¹Ø±Ø¶ Ø£Ø¹Ù„Ù‰ {top_k if top_k else 'Ø§Ù„ÙƒÙ„'} Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªØ¨Ø©:")

    for rank, (doc_id, score) in enumerate(sorted_ranking[:5], 1):
        print(f"    {rank}. Doc ID: {doc_id}, Score: {score:.6f}")

    return OrderedDict(sorted_ranking)
    print("[1] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…...")
    qp = QueryProcessor(dataset_name)
    query_vector, tokens = qp.process(query)
    print(f"[2] ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {tokens}")

    print("[3] ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF...")
    tfidf_matrix = load_tfidf_matrix(f"{dataset_name}_all")
    print(f"[4] Ø´ÙƒÙ„ Ù…ØµÙÙˆÙØ© TF-IDF: {tfidf_matrix.shape}")

    print("[5] ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
    doc_ids = load_doc_ids(f"{dataset_name}_all")
    print(f"[6] Ø¹ÙŠÙ†Ø© Ù…Ù† doc_ids: {doc_ids[:5]} (ÙƒÙ„Ù‡Ø§ Ù…Ù† Ù†ÙˆØ¹: {type(doc_ids[0])})")

    print("[7] ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³...")
    inverted_index = load_inverted_index(dataset_name)
    print(f"[8] Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {len(inverted_index)}")

    matched_tokens = [t for t in tokens if t in inverted_index]
    print(f"[9] Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {matched_tokens}")

    if not matched_tokens:
        print("[!] Ù„Ù… ØªÙÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        return OrderedDict()

    candidate_doc_ids = set()
    for token in matched_tokens:
        candidate_doc_ids.update(inverted_index[token])
    print(f"[10] Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ: {len(candidate_doc_ids)}")

    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}
    print(f"[11] ØªÙ… Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨ÙŠÙ† doc_id Ùˆ index.")

    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]
    print(f"[12] Ø¹Ø¯Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {len(candidate_indices)}")

    if not candidate_indices:
        print("[!] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø·Ø§Ø¨Ù‚Ø©. Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©.")
        return OrderedDict()

    candidate_indices = sorted(candidate_indices)
    candidate_vectors = tfidf_matrix[candidate_indices]
    print(f"[13] ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø©. Ø§Ù„Ø´ÙƒÙ„: {candidate_vectors.shape}")

    similarity_scores = cosine_similarity(query_vector, candidate_vectors).flatten()
    print(f"[14] ØªÙ… Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªØ´Ø§Ø¨Ù‡. Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: {len(similarity_scores)}")

    ranking = {
        doc_ids[i]: float(score)
        for i, score in zip(candidate_indices, similarity_scores)
        if score >= similarity_threshold
    }
    print(f"[15] Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ({similarity_threshold}): {len(ranking)}")

    sorted_ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)

    if top_k:
        sorted_ranking = sorted_ranking[:top_k]
    print(f"[16] Ø¹Ø±Ø¶ Ø£Ø¹Ù„Ù‰ {top_k if top_k else 'Ø§Ù„ÙƒÙ„'} Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªØ¨Ø©:")

    for rank, (doc_id, score) in enumerate(sorted_ranking[:5], 1):
        print(f"    {rank}. Doc ID: {doc_id}, Score: {score:.6f}")

    return OrderedDict(sorted_ranking)
    qp = QueryProcessor(dataset_name)
    query_vector, tokens = qp.process(query)

    print("âœ… Step 1 - Tokens after preprocessing:", tokens)

    tfidf_matrix = load_tfidf_matrix(f"{dataset_name}_all")
    doc_ids = load_doc_ids(f"{dataset_name}_all")
    inverted_index = load_inverted_index(dataset_name)

    print("âœ… Step 2 - Checking which tokens exist in the inverted index:")
    matched_tokens = [t for t in tokens if t in inverted_index]
    print("âœ… Tokens found in index:", matched_tokens)

    if not matched_tokens:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³.")
        return OrderedDict()

    # ØªØ­ÙˆÙŠÙ„ doc_id Ø¥Ù„Ù‰ index
    doc_id_to_index = {str(doc_id): idx for idx, doc_id in enumerate(doc_ids)}
    print("âœ… Tokens found in index:", doc_id_to_index)

    candidate_doc_indices = set()
    for token in matched_tokens:
        for doc_id in inverted_index[token]:
            index = doc_id_to_index.get(doc_id)
            if index is not None:
                candidate_doc_indices.add(index)

    print("âœ… Step 3 - Number of candidate documents found:", len(candidate_doc_indices))

    if not candidate_doc_indices:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø±Ø´Ø­Ø©.")
        return OrderedDict()

    candidate_doc_indices = sorted(candidate_doc_indices)
    candidate_doc_vectors = tfidf_matrix[candidate_doc_indices]

    print("âœ… Step 4 - Calculating cosine similarity...")
    similarity_scores = cosine_similarity(query_vector, candidate_doc_vectors).flatten()

    print("âœ… Step 5 - Building ranked result...")
    ranking = {
        doc_ids[i]: float(score)
        for i, score in zip(candidate_doc_indices, similarity_scores)
        if score >= similarity_threshold
    }

    print("âœ… Step 6 - Number of documents above threshold:", len(ranking))

    sorted_ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)

    if top_k:
        sorted_ranking = sorted_ranking[:top_k]

    print("âœ… Step 7 - Top results preview:", sorted_ranking[:5])

    return OrderedDict(sorted_ranking)